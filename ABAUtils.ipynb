{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21faf61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "'''\n",
    "Plot loss and metrics of keras training.\n",
    "'''\n",
    "def utils_plot_keras_training(training):\n",
    "    metrics = [k for k in training.history.keys() if (\"loss\" not in k) and (\"val\" not in k)]\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2, sharey=True, figsize=(15,3))\n",
    "    \n",
    "    ## training\n",
    "    ax[0].set(title=\"Training\")\n",
    "    ax11 = ax[0].twinx()\n",
    "    ax[0].plot(training.history['loss'], color='black')\n",
    "    ax[0].set_xlabel('Epochs')\n",
    "    ax[0].set_ylabel('Loss', color='black')\n",
    "    for metric in metrics:\n",
    "        ax11.plot(training.history[metric], label=metric)\n",
    "    ax11.set_ylabel(\"Score\", color='steelblue')\n",
    "    ax11.legend()\n",
    "    \n",
    "    ## validation\n",
    "    ax[1].set(title=\"Validation\")\n",
    "    ax22 = ax[1].twinx()\n",
    "    ax[1].plot(training.history['val_loss'], color='black')\n",
    "    ax[1].set_xlabel('Epochs')\n",
    "    ax[1].set_ylabel('Loss', color='black')\n",
    "    for metric in metrics:\n",
    "        ax22.plot(training.history['val_'+metric], label=metric)\n",
    "    ax22.set_ylabel(\"Score\", color=\"steelblue\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f2e57e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'testf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1624/1490484767.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mypred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtestf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'yhat'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mytrue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtestf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'overall'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mcutoff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'testf' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "\n",
    "def test(ytrue, ypred, ydiff, cutoff):\n",
    "    r2_model=r2_score(ytrue, ypred) # Calculate r2_score of predictions\n",
    "\n",
    "    squarederror_model = (ytrue-ypred)**2 # Calculate SE of predictions\n",
    "    MSE_model=squarederror_model.mean() # MSE\n",
    "\n",
    "    abserror_model = np.abs(ytrue-ypred) # Calculate AE of predictions\n",
    "    MAE_model = abserror_model.mean()\n",
    "\n",
    "    APE_model = 100*np.abs((ytrue-ypred)/ytrue) # Calculate APE of predictions\n",
    "    MAPE_model = APE_model.mean()\n",
    "    positives_obs = (ytrue>=cutoff).sum()\n",
    "    negatives_obs = (ytrue<cutoff).sum()\n",
    "    positives_pred = (ypred>=cutoff).sum()\n",
    "    negatives_pred = (ypred<cutoff).sum()\n",
    "\n",
    "    tn = np.sum((ytrue<cutoff)&(ypred<cutoff))\n",
    "    tp = np.sum((ytrue>=cutoff)&(ypred>=cutoff))\n",
    "    fn = np.sum((ytrue>=cutoff)&(ypred<cutoff))\n",
    "    fp = np.sum((ytrue<cutoff)&(ypred>=cutoff))\n",
    "\n",
    "    #perc=test[test['overall']>=4].shape[0]/test.shape[0]\n",
    "    #q=test['yhat'].quantile(1-perc)\n",
    "    #positives_c = test[test['yhat']>=q].shape[0]\n",
    "    #negatives_c = test[test['yhat']<q].shape[0]\n",
    "\n",
    "    accuracy = (tp+tn)/(tp+tn+fp+fn)\n",
    "    precision = tp/(tp+fp)\n",
    "    recall = tp/(tp+fn)\n",
    "    f1=2*(precision*recall)/(precision+recall)\n",
    "    specificity = tn/(tn+fp)\n",
    "    \n",
    "    # Hvor mange er forudsagt mindre end de faktisk er\n",
    "    under_modeln=np.sum(ydiff<0)\n",
    "\n",
    "    # Hvor mange er forudsagt større end de faktisk er\n",
    "    over_modeln=np.sum(ydiff>0)\n",
    "    \n",
    "    # Lige på\n",
    "    equal_modeln = np.sum(ydiff==0)\n",
    "\n",
    "\n",
    "    KPI_model = np.array([r2_model, MSE_model, MAE_model, MAPE_model, under_modeln, over_modeln, equal_modeln,\n",
    "                         tp, tn, fp, fn, accuracy, precision, recall, f1, specificity])\n",
    "    KPIdf=pd.DataFrame(KPI_model).T\n",
    "    KPIdf.columns=['r2', 'MSE', 'MAE', 'MAPE', 'pred<true', 'pred>true', 'equal', 'TP', 'TN', 'FP', 'FN',\n",
    "                  'Accuracy', 'Precision', 'Recall', 'f1', 'Specificity']\n",
    "\n",
    "\n",
    "    return KPIdf, KPI_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1162520-1bdb-49d2-9e86-2ecbe90bb4fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
